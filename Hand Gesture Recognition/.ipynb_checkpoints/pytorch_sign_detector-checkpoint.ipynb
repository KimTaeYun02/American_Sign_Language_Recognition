{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b75fa3a",
   "metadata": {},
   "source": [
    "# Pytorch를 활용한 hand sign 검출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "bfdbeee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt5Agg\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "import copy\n",
    "import glob\n",
    "from tqdm import tqdm as tqdmd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import transforms\n",
    "from torchvision.io import read_image\n",
    "import torchvision.models as models\n",
    "from torchsummary import summary\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db042147",
   "metadata": {},
   "source": [
    "## Dataset Review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abeeddd5",
   "metadata": {},
   "source": [
    "![ASL](./image/01.png)\n",
    "* ASL(American Sign Language) alphabet은 총 26개가 있지만 dataset에서는 25개 밖에 없다. \n",
    "* 그 이유는 Z가 검지로 Z를 그리는 것이기 때문에 사진으로는 표현이 불가능."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724aa0db",
   "metadata": {},
   "source": [
    "## Image dir, path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "30b50388",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DIR = \"Train/\"\n",
    "TEST_DIR = \"Test/\"\n",
    "TRAIN_DIR_LIST = os.listdir(TRAIN_DIR)\n",
    "TEST_DIR_LIST = os.listdir(TEST_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d70a3527",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Trian Dataset\n",
    "df = pd.DataFrame(columns = ['label', 'dir'])\n",
    "for i in TRAIN_DIR_LIST:\n",
    "    for j in os.listdir(TRAIN_DIR + i +\"/\"):\n",
    "        data = {'label' : i, 'dir' : TRAIN_DIR + i + \"/\" + j }\n",
    "        df = df.append(data, ignore_index=True)\n",
    "df.to_csv(\"Train/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "31d005ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns = ['label', 'dir'])\n",
    "for i in TEST_DIR_LIST:\n",
    "    for j in os.listdir(TEST_DIR + i +\"/\"):\n",
    "        data = {'label' : i, 'dir' : TEST_DIR + i + \"/\" + j }\n",
    "        df = df.append(data, ignore_index=True)\n",
    "df.to_csv(\"Test/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f86f04c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7172 entries, 0 to 7171\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   label   7172 non-null   object\n",
      " 1   dir     7172 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 112.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "0d1d90d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = pd.read_csv(\"Train/train.csv\", index_col = 0)\n",
    "test_dataset = pd.read_csv(\"Test/test.csv\", index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "dc2c5a2c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>dir</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>Train/A/10014_A.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A</td>\n",
       "      <td>Train/A/10021_A.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A</td>\n",
       "      <td>Train/A/10023_A.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A</td>\n",
       "      <td>Train/A/10050_A.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A</td>\n",
       "      <td>Train/A/10056_A.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                  dir\n",
       "0     A  Train/A/10014_A.jpg\n",
       "1     A  Train/A/10021_A.jpg\n",
       "2     A  Train/A/10023_A.jpg\n",
       "3     A  Train/A/10050_A.jpg\n",
       "4     A  Train/A/10056_A.jpg"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "b70c75e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "R    1294\n",
       "Q    1279\n",
       "L    1241\n",
       "W    1225\n",
       "F    1204\n",
       "S    1199\n",
       "D    1196\n",
       "O    1196\n",
       "T    1186\n",
       "X    1164\n",
       "I    1162\n",
       "U    1161\n",
       "N    1151\n",
       "C    1144\n",
       "A    1126\n",
       "Y    1118\n",
       "K    1114\n",
       "G    1090\n",
       "P    1088\n",
       "V    1082\n",
       "M    1055\n",
       "H    1013\n",
       "B    1010\n",
       "E     957\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fbcafc",
   "metadata": {},
   "source": [
    "## Data Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "f0f62e90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        A\n",
       "1        A\n",
       "2        A\n",
       "3        A\n",
       "4        A\n",
       "        ..\n",
       "27450    Y\n",
       "27451    Y\n",
       "27452    Y\n",
       "27453    Y\n",
       "27454    Y\n",
       "Name: label, Length: 27455, dtype: object"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.iloc[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "613a3900",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-116-f6bf5d23faac>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-116-f6bf5d23faac>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    train_dataset.loc[,][0]\u001b[0m\n\u001b[1;37m                      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "train_dataset.loc[,][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d3457812",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(train_dataset.loc[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "52417549",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "cf49a256",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAACsklEQVR4nCXOy4ocVRgA4P//z3/q0vceeqZHjWMSh6CRZCMJgYC4EBcudKUvID6DK0F8jeyD4EYkixhEJSCixEkCgWScpOmYmcxUd09fqqqr6tS5ucj3BB/eQUQAD+jDPF4HlkCxA5mRQM0CADyAd85yLBz4POrMfN2UqY5JEAIAIMQC12XuZKOaYfzCpY0+MgAAIgIFHpCmo/Ls6X+bZ/ZbA+eRPCARkYg0GIDi4U93kqc/P3b3z1QTYngV8pD2jkbLQdS5t9F9vDr7/dVuuyQSQpC3+vDBcrp3a771HuNGL2nIHyZ9IkGC0Fvz5LenOABSaU+acC+5/uyPJ20CXWtJq3W/nzQ3dvfbWRVU1VEdX7w9KEiGEo3KTxqvzYtjlUN/ejcfd85h8rYgMkSMiMtyuwgpWE3N819+P7rSbBxfE5INA2rtlrP60U57Pl6kMOx++MnzY30FBEuuRZXpTCUHd3ez9bT12XkrA/fv9lunwIw+FNgf/D2nf6ZJ+PpHH6zo8P723ueBBQZjWIj+GxdWO5PNS8OLUZQ0zXg/u2bYMWvtVNERWwu//f7uuhXmYnQ4u/fFBRsp1thexagld+vhZshosoWav3jnK+U9E6CXsk4Zo1ZTApBN1Gxkvoucc46BnBCwasVdbEunzXg5PXn59Y4FBGAbGxv2TmzLaLIq00fFyz+//Dh17L1nL1D74UJ5W/mcT7M0+evqp76dAgIwGUTTCbOyKE2Gk9Pk191vEC1b8J5lSXHNQAoDqtVsetD5dmjSWAsPwDKP41Jppsh5rPJsdPOyKlsust4D3xA9vx68CVsZ13mejy/B4ckmVQEACR4rru27w96CqF7OU3XwI827xjWlB2QhA+HiMPCE5WyyLM7la12XFDprNEvZxjrUKZIrs7xU57sNhoKU0br6H3JTeI5v3PZ8AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.JpegImagePlugin.JpegImageFile image mode=L size=28x28 at 0x1C3B4BA3430>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457a5235",
   "metadata": {},
   "source": [
    "## Mapping\n",
    "* A ~ Y -> 0 ~ 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "adc3e2d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mapping = {}\n",
    "for i in range(25):\n",
    "    mapping[chr(65+i)] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "6762829c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.replace(mapping)\n",
    "test_dataset = test_dataset.replace(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "69e68760",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17    1294\n",
       "16    1279\n",
       "11    1241\n",
       "22    1225\n",
       "5     1204\n",
       "18    1199\n",
       "14    1196\n",
       "3     1196\n",
       "19    1186\n",
       "23    1164\n",
       "8     1162\n",
       "20    1161\n",
       "13    1151\n",
       "2     1144\n",
       "0     1126\n",
       "24    1118\n",
       "10    1114\n",
       "6     1090\n",
       "15    1088\n",
       "21    1082\n",
       "12    1055\n",
       "7     1013\n",
       "1     1010\n",
       "4      957\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a232c2a9",
   "metadata": {},
   "source": [
    "## transformer\n",
    "* RandomHorizontalFlip(p)\n",
    "\n",
    "\n",
    "Horizontally flip the given image randomly with a given probability.\n",
    "* RandomRotation(degrees)\n",
    "\n",
    "\n",
    "Rotate the image by angle.\n",
    "* RandomVerticalFlip(p)\n",
    "\n",
    "\n",
    "Vertically flip the given image randomly with a given probability.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "79b840d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIM = (28, 28)\n",
    "NUM_CLASSES = 25\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#color img\n",
    "#MEAN = [0.485, 0.456, 0.406]\n",
    "#STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "#gray img\n",
    "MEAN = [0.5]\n",
    "STD = [0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "e8f9fa3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(value = 'val'):\n",
    "    if value == 'train':\n",
    "            transform = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize(DIM),\n",
    "            transforms.RandomHorizontalFlip(p = 0.5),\n",
    "            transforms.RandomRotation(90),\n",
    "            transforms.RandomVerticalFlip(p = 0.5),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(MEAN, STD)\n",
    "        ])\n",
    "            return transform\n",
    "    \n",
    "    elif value == 'val':\n",
    "            transform = torchvision.transforms.Compose([\n",
    "            torchvision.transforms.ToPILImage(),\n",
    "            torchvision.transforms.Resize(DIM),\n",
    "            torchvision.transforms.ToTensor(),\n",
    "            torchvision.transforms.Normalize(MEAN, STD)\n",
    "        ])\n",
    "            return transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07ba899",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "8bf7cfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASLDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, dataset, transform=None):\n",
    "        self.labels = dataset.iloc[:, 0]\n",
    "        self.file_dir = dataset.iloc[:, 1]\n",
    "        self.transform = transform\n",
    "        \n",
    "    \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = read_image(os.path.join(self.file_dir[idx]))\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        label = torch.tensor(self.labels[idx], dtype = torch.long)\n",
    "        # image, label을 return\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7821ef79",
   "metadata": {},
   "source": [
    "## Check DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "4cf5d076",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          ...,\n",
      "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000]]],\n",
      "\n",
      "\n",
      "        [[[-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000,  0.3882,  ...,  0.2784, -1.0000, -1.0000],\n",
      "          ...,\n",
      "          [-1.0000, -1.0000,  0.3333,  ...,  0.4902, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000]]],\n",
      "\n",
      "\n",
      "        [[[-1.0000,  0.7255,  0.7098,  ...,  0.6157, -1.0000, -1.0000],\n",
      "          [-1.0000,  0.6941,  0.7098,  ...,  0.6157,  0.6000,  0.5922],\n",
      "          [ 0.6392,  0.6706,  0.7098,  ...,  0.6078,  0.6000,  0.5922],\n",
      "          ...,\n",
      "          [ 0.6000,  0.6000,  0.6078,  ...,  0.4667,  0.4980,  0.5294],\n",
      "          [ 0.5922,  0.5922,  0.6000,  ...,  0.4745,  0.4588, -1.0000],\n",
      "          [-1.0000, -1.0000,  0.5922,  ...,  0.4980,  0.4588, -1.0000]]],\n",
      "\n",
      "\n",
      "        [[[-1.0000, -1.0000, -1.0000,  ...,  0.1137, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000,  ...,  0.1216, -1.0000, -1.0000],\n",
      "          [-0.0667, -0.0510, -0.0275,  ...,  0.1373, -1.0000, -1.0000],\n",
      "          ...,\n",
      "          [-1.0000, -1.0000,  0.0980,  ...,  0.3176,  0.3412,  0.3412],\n",
      "          [-1.0000, -1.0000,  0.0824,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000,  0.0745,  ..., -1.0000, -1.0000, -1.0000]]]])\n",
      "tensor([15, 12,  0, 14])\n"
     ]
    }
   ],
   "source": [
    "dataset = ASLDataset(dataset = train_dataset, transform = transform_data(\"train\"))\n",
    "dataLoader = torch.utils.data.DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "dataiter = iter(dataLoader)\n",
    "feature, target = dataiter.next()\n",
    "print(feature)\n",
    "print(target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191310f2",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac86e91",
   "metadata": {},
   "source": [
    "RestNet-152\n",
    "![ASL](./image/02.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da0a5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModel():\n",
    "    net = models.resnet152(pretrained=True)\n",
    "    \n",
    "    # if you want to train the whole network, comment this code\n",
    "    # freeze all the layers in the network\n",
    "    for param in net.parameters():\n",
    "        param.requires_grad = False\n",
    "        \n",
    "    num_ftrs = net.fc.in_features\n",
    "    # create last few layers\n",
    "    net.fc = nn.Sequential(\n",
    "        nn.Linear(num_ftrs, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.3),\n",
    "        nn.Linear(256, NUM_CLASSES),\n",
    "        nn.LogSoftmax(dim=1)\n",
    "    )\n",
    "    \n",
    "    # use gpu if any\n",
    "    net = net.cuda() if DEVICE else net\n",
    "    return net\n",
    "ASL_classification = getModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ee01a0",
   "metadata": {},
   "source": [
    "## Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bf7648",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(cassava_classification, input_size = (1,28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd66863",
   "metadata": {},
   "source": [
    "## Cyclical Learning Rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7759b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def cyclical_lr(stepsize, min_lr=3e-4, max_lr=3e-3):\n",
    "\n",
    "    # Scaler: we can adapt this if we do not want the triangular CLR\n",
    "    scaler = lambda x: 1.\n",
    "\n",
    "    # Lambda function to calculate the LR\n",
    "    lr_lambda = lambda it: min_lr + (max_lr - min_lr) * relative(it, stepsize)\n",
    "\n",
    "    # Additional function to see where on the cycle we are\n",
    "    def relative(it, stepsize):\n",
    "        cycle = math.floor(1 + it / (2 * stepsize))\n",
    "        x = abs(it / stepsize - 2 * cycle + 1)\n",
    "        return max(0, (1 - x)) * scaler(cycle)\n",
    "\n",
    "    return lr_lambda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958758de",
   "metadata": {},
   "source": [
    "## Loss Function, Optimizer, scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9652be",
   "metadata": {},
   "source": [
    "#### torch.nn.CrossEntropyLoss\n",
    "![Loss](./image/03.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f44728",
   "metadata": {},
   "source": [
    "torch.optim.SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8991b8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "model     = ASL_classification\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1., momentum=0.9)\n",
    "step_size = 4*len(train_loader)\n",
    "clr = cyclical_lr(step_size, min_lr=3e-4, max_lr=3e-3)\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, [clr])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

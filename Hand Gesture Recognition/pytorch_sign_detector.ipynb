{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b75fa3a",
   "metadata": {},
   "source": [
    "# Pytorch를 활용한 Sign Language 검출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "bfdbeee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt5Agg\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "import copy\n",
    "import glob\n",
    "from tqdm import tqdm as tqdmd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import transforms\n",
    "from torchvision.io import read_image\n",
    "import torchvision.models as models\n",
    "from torchsummary import summary\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db042147",
   "metadata": {},
   "source": [
    "## Dataset Preview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abeeddd5",
   "metadata": {},
   "source": [
    "![ASL](./image/01.png)\n",
    "* ASL(American Sign Language) alphabet은 총 26개가 있지만 dataset에서는 25개 밖에 없다. \n",
    "* 그 이유는 Z가 검지로 Z를 그리는 것이기 때문에 사진으로는 표현이 불가능."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b458779a",
   "metadata": {},
   "source": [
    "## Image dir, path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "30b50388",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DIR = \"Train/\"\n",
    "TEST_DIR = \"Test/\"\n",
    "TRAIN_DIR_LIST = os.listdir(TRAIN_DIR)\n",
    "TEST_DIR_LIST = os.listdir(TEST_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a2f45a81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Trian Dataset\n",
    "df = pd.DataFrame(columns = ['label', 'dir'])\n",
    "for i in TRAIN_DIR_LIST:\n",
    "    for j in os.listdir(TRAIN_DIR + i +\"/\"):\n",
    "        data = {'label' : i, 'dir' : TRAIN_DIR + i + \"/\" + j }\n",
    "        df = df.append(data, ignore_index=True)\n",
    "df.to_csv(\"Train/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "eba3997f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns = ['label', 'dir'])\n",
    "for i in TEST_DIR_LIST:\n",
    "    for j in os.listdir(TEST_DIR + i +\"/\"):\n",
    "        data = {'label' : i, 'dir' : TEST_DIR + i + \"/\" + j }\n",
    "        df = df.append(data, ignore_index=True)\n",
    "df.to_csv(\"Test/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3322121d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7172 entries, 0 to 7171\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   label   7172 non-null   object\n",
      " 1   dir     7172 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 112.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "45892d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = pd.read_csv(\"Train/train.csv\", index_col = 0)\n",
    "test_dataset = pd.read_csv(\"Test/test.csv\", index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "56d0c0f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>dir</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>Train/A/10014_A.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A</td>\n",
       "      <td>Train/A/10021_A.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A</td>\n",
       "      <td>Train/A/10023_A.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A</td>\n",
       "      <td>Train/A/10050_A.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A</td>\n",
       "      <td>Train/A/10056_A.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                  dir\n",
       "0     A  Train/A/10014_A.jpg\n",
       "1     A  Train/A/10021_A.jpg\n",
       "2     A  Train/A/10023_A.jpg\n",
       "3     A  Train/A/10050_A.jpg\n",
       "4     A  Train/A/10056_A.jpg"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "ff0eec0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "R    1294\n",
       "Q    1279\n",
       "L    1241\n",
       "W    1225\n",
       "F    1204\n",
       "S    1199\n",
       "D    1196\n",
       "O    1196\n",
       "T    1186\n",
       "X    1164\n",
       "I    1162\n",
       "U    1161\n",
       "N    1151\n",
       "C    1144\n",
       "A    1126\n",
       "Y    1118\n",
       "K    1114\n",
       "G    1090\n",
       "P    1088\n",
       "V    1082\n",
       "M    1055\n",
       "H    1013\n",
       "B    1010\n",
       "E     957\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377d798e",
   "metadata": {},
   "source": [
    "## Data Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "6d8e41dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        A\n",
       "1        A\n",
       "2        A\n",
       "3        A\n",
       "4        A\n",
       "        ..\n",
       "27450    Y\n",
       "27451    Y\n",
       "27452    Y\n",
       "27453    Y\n",
       "27454    Y\n",
       "Name: label, Length: 27455, dtype: object"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.iloc[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6dd548d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(train_dataset.loc[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "70b95c94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "dd834072",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAACsklEQVR4nCXOy4ocVRgA4P//z3/q0vceeqZHjWMSh6CRZCMJgYC4EBcudKUvID6DK0F8jeyD4EYkixhEJSCixEkCgWScpOmYmcxUd09fqqqr6tS5ucj3BB/eQUQAD+jDPF4HlkCxA5mRQM0CADyAd85yLBz4POrMfN2UqY5JEAIAIMQC12XuZKOaYfzCpY0+MgAAIgIFHpCmo/Ls6X+bZ/ZbA+eRPCARkYg0GIDi4U93kqc/P3b3z1QTYngV8pD2jkbLQdS5t9F9vDr7/dVuuyQSQpC3+vDBcrp3a771HuNGL2nIHyZ9IkGC0Fvz5LenOABSaU+acC+5/uyPJ20CXWtJq3W/nzQ3dvfbWRVU1VEdX7w9KEiGEo3KTxqvzYtjlUN/ejcfd85h8rYgMkSMiMtyuwgpWE3N819+P7rSbBxfE5INA2rtlrP60U57Pl6kMOx++MnzY30FBEuuRZXpTCUHd3ez9bT12XkrA/fv9lunwIw+FNgf/D2nf6ZJ+PpHH6zo8P723ueBBQZjWIj+GxdWO5PNS8OLUZQ0zXg/u2bYMWvtVNERWwu//f7uuhXmYnQ4u/fFBRsp1thexagld+vhZshosoWav3jnK+U9E6CXsk4Zo1ZTApBN1Gxkvoucc46BnBCwasVdbEunzXg5PXn59Y4FBGAbGxv2TmzLaLIq00fFyz+//Dh17L1nL1D74UJ5W/mcT7M0+evqp76dAgIwGUTTCbOyKE2Gk9Pk191vEC1b8J5lSXHNQAoDqtVsetD5dmjSWAsPwDKP41Jppsh5rPJsdPOyKlsust4D3xA9vx68CVsZ13mejy/B4ckmVQEACR4rru27w96CqF7OU3XwI827xjWlB2QhA+HiMPCE5WyyLM7la12XFDprNEvZxjrUKZIrs7xU57sNhoKU0br6H3JTeI5v3PZ8AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.JpegImagePlugin.JpegImageFile image mode=L size=28x28 at 0x1C3B4BA3430>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307f703f",
   "metadata": {},
   "source": [
    "## Mapping\n",
    "* A ~ Y -> 0 ~ 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "d025218d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mapping = {}\n",
    "for i in range(25):\n",
    "    mapping[chr(65+i)] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "a2b283d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.replace(mapping)\n",
    "test_dataset = test_dataset.replace(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "2bbef4cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17    1294\n",
       "16    1279\n",
       "11    1241\n",
       "22    1225\n",
       "5     1204\n",
       "18    1199\n",
       "14    1196\n",
       "3     1196\n",
       "19    1186\n",
       "23    1164\n",
       "8     1162\n",
       "20    1161\n",
       "13    1151\n",
       "2     1144\n",
       "0     1126\n",
       "24    1118\n",
       "10    1114\n",
       "6     1090\n",
       "15    1088\n",
       "21    1082\n",
       "12    1055\n",
       "7     1013\n",
       "1     1010\n",
       "4      957\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d278c562",
   "metadata": {},
   "source": [
    "## transformer\n",
    "* RandomHorizontalFlip(p)\n",
    "\n",
    "\n",
    "Horizontally flip the given image randomly with a given probability.\n",
    "* RandomRotation(degrees)\n",
    "\n",
    "\n",
    "Rotate the image by angle.\n",
    "* RandomVerticalFlip(p)\n",
    "\n",
    "\n",
    "Vertically flip the given image randomly with a given probability.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "a6a4480c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIM = (28, 28)\n",
    "NUM_CLASSES = 25\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#color img\n",
    "#MEAN = [0.485, 0.456, 0.406]\n",
    "#STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "#gray img\n",
    "MEAN = [0.5]\n",
    "STD = [0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "6bdc11e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(value = 'val'):\n",
    "    if value == 'train':\n",
    "            transform = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize(DIM),\n",
    "            transforms.RandomHorizontalFlip(p = 0.5),\n",
    "            transforms.RandomRotation(90),\n",
    "            transforms.RandomVerticalFlip(p = 0.5),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(MEAN, STD)\n",
    "        ])\n",
    "            return transform\n",
    "    \n",
    "    elif value == 'val':\n",
    "            transform = torchvision.transforms.Compose([\n",
    "            torchvision.transforms.ToPILImage(),\n",
    "            torchvision.transforms.Resize(DIM),\n",
    "            torchvision.transforms.ToTensor(),\n",
    "            torchvision.transforms.Normalize(MEAN, STD)\n",
    "        ])\n",
    "            return transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e3a6f9",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "7ae066f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASLDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, dataset, transform=None):\n",
    "        self.labels = dataset.iloc[:, 0]\n",
    "        self.file_dir = dataset.iloc[:, 1]\n",
    "        self.transform = transform\n",
    "        \n",
    "    \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = read_image(os.path.join(self.file_dir[idx]))\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        label = torch.tensor(self.labels[idx], dtype = torch.long)\n",
    "        # image, label을 return\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee3731e",
   "metadata": {},
   "source": [
    "## Check DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "3c3fdc73",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          ...,\n",
      "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000]]],\n",
      "\n",
      "\n",
      "        [[[-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000,  0.3882,  ...,  0.2784, -1.0000, -1.0000],\n",
      "          ...,\n",
      "          [-1.0000, -1.0000,  0.3333,  ...,  0.4902, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000]]],\n",
      "\n",
      "\n",
      "        [[[-1.0000,  0.7255,  0.7098,  ...,  0.6157, -1.0000, -1.0000],\n",
      "          [-1.0000,  0.6941,  0.7098,  ...,  0.6157,  0.6000,  0.5922],\n",
      "          [ 0.6392,  0.6706,  0.7098,  ...,  0.6078,  0.6000,  0.5922],\n",
      "          ...,\n",
      "          [ 0.6000,  0.6000,  0.6078,  ...,  0.4667,  0.4980,  0.5294],\n",
      "          [ 0.5922,  0.5922,  0.6000,  ...,  0.4745,  0.4588, -1.0000],\n",
      "          [-1.0000, -1.0000,  0.5922,  ...,  0.4980,  0.4588, -1.0000]]],\n",
      "\n",
      "\n",
      "        [[[-1.0000, -1.0000, -1.0000,  ...,  0.1137, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000, -1.0000,  ...,  0.1216, -1.0000, -1.0000],\n",
      "          [-0.0667, -0.0510, -0.0275,  ...,  0.1373, -1.0000, -1.0000],\n",
      "          ...,\n",
      "          [-1.0000, -1.0000,  0.0980,  ...,  0.3176,  0.3412,  0.3412],\n",
      "          [-1.0000, -1.0000,  0.0824,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-1.0000, -1.0000,  0.0745,  ..., -1.0000, -1.0000, -1.0000]]]])\n",
      "tensor([15, 12,  0, 14])\n"
     ]
    }
   ],
   "source": [
    "dataset = ASLDataset(dataset = train_dataset, transform = transform_data(\"train\"))\n",
    "dataLoader = torch.utils.data.DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "dataiter = iter(dataLoader)\n",
    "feature, target = dataiter.next()\n",
    "print(feature)\n",
    "print(target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9d6b90",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c2e0f1",
   "metadata": {},
   "source": [
    "RestNet-152\n",
    "![ASL](./image/02.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd76258",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModel():\n",
    "    net = models.resnet152(pretrained=True)\n",
    "    \n",
    "    # if you want to train the whole network, comment this code\n",
    "    # freeze all the layers in the network\n",
    "    for param in net.parameters():\n",
    "        param.requires_grad = False\n",
    "        \n",
    "    num_ftrs = net.fc.in_features\n",
    "    # create last few layers\n",
    "    net.fc = nn.Sequential(\n",
    "        nn.Linear(num_ftrs, 256),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.3),\n",
    "        nn.Linear(256, NUM_CLASSES),\n",
    "        nn.LogSoftmax(dim=1)\n",
    "    )\n",
    "    \n",
    "    # use gpu if any\n",
    "    net = net.cuda() if DEVICE else net\n",
    "    return net\n",
    "ASL_classification = getModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3842279f",
   "metadata": {},
   "source": [
    "## Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b28c577",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(cassava_classification, input_size = (1,28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d3af0c",
   "metadata": {},
   "source": [
    "## Cyclical Learning Rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667d831c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def cyclical_lr(stepsize, min_lr=3e-4, max_lr=3e-3):\n",
    "\n",
    "    # Scaler: we can adapt this if we do not want the triangular CLR\n",
    "    scaler = lambda x: 1.\n",
    "\n",
    "    # Lambda function to calculate the LR\n",
    "    lr_lambda = lambda it: min_lr + (max_lr - min_lr) * relative(it, stepsize)\n",
    "\n",
    "    # Additional function to see where on the cycle we are\n",
    "    def relative(it, stepsize):\n",
    "        cycle = math.floor(1 + it / (2 * stepsize))\n",
    "        x = abs(it / stepsize - 2 * cycle + 1)\n",
    "        return max(0, (1 - x)) * scaler(cycle)\n",
    "\n",
    "    return lr_lambda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9d630d",
   "metadata": {},
   "source": [
    "## Loss Function, Optimizer, scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f033702",
   "metadata": {},
   "source": [
    "#### torch.nn.CrossEntropyLoss\n",
    "![Loss](./image/03.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2972a2b5",
   "metadata": {},
   "source": [
    "#### torch.optim.Adam\n",
    "\n",
    "__1. 개요__\n",
    "\n",
    "model을 학습시키기 위해선 optimization라는 작업을 해줌. weight의 torch.backward()라는 함수로, gradient를 구해서 weight의 값을 변화시켜주는 역활을 함.\n",
    "\n",
    "__2. Adam__\n",
    "![Adam](./image/04.png)\n",
    "Adam method는 Adagrad + RMSProp의 장점을 섞어 놓은 것으로 __stepsize가 gradient의 rescaling에 영향을 받지 않음.__ 즉, gradient가 커져도 stepsize는 bound되어 있어서 어떠한 object function을 사용하더라도 안정적으로 optimization을 위한 하강이 가능하다.\n",
    "\n",
    "Adam pseudo-code\n",
    "![Adama_pesudo_code](./image/05.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd52d344",
   "metadata": {},
   "source": [
    "* AdaGrad란?\n",
    "\n",
    "\n",
    "AgaGrad는 과거의 gradient 변화량을 참고한다. 예를 들어, 이미 많이 변화한 변수들은 optimum에 거의 도달했다 보고 stepsize를 작게하는 반면, 여태까지 많이 변화하지 않은 변수들은 optimum에 도달하지 않았다고 보고 stepsize를 크게 한다.\n",
    "![AdaGrad](./image/06.png)\n",
    "하지만 iteration이 계속될 수록 stepsize가 너무 작아질 수 있는 문제점이 존재한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89a683e",
   "metadata": {},
   "source": [
    "* RMSProp란?\n",
    "\n",
    "\n",
    "AdaGrad의 stepsize와 관련된 문제점을 해결한 것이 RMSProp이다. RMSProp은 [exponential moving average](https://www.investopedia.com/ask/answers/122314/what-exponential-moving-average-ema-formula-and-how-ema-calculated.asp)를 사용하여 과거의 정보에 가중치를 작게, 최근 값에 민감하도록 가중치를 크게 부여하는 형태이다.\n",
    "![AdaGrad](./image/07.png)\n",
    "하지만 G 변수의 bias를 수정해주지 않으면 B2가 1에 가까워질수록 매우 큰 stepsize를 갖거나 발산하게 되는 문제점이 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3e5b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "model     = ASL_classification\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "step_size = 4*len(train_loader)\n",
    "clr = cyclical_lr(step_size, min_lr=3e-4, max_lr=3e-3)\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, [clr])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
